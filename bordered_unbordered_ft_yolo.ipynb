{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Keremberke yolo8m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/ultralytics/ultralytics.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!git lfs install must be installed first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov8m-table-extraction' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#downloading keremberk models\n",
    "\n",
    "!git clone https://huggingface.co/keremberke/yolov8m-table-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning keremberke8M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 12, 19, 14, 13, 13, 898914)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suparna/anaconda3/envs/idp_0/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Ultralytics YOLOv8.0.43 ðŸš€ Python-3.8.18 torch-2.1.1+cu121 CPU\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=/home/suparna/PycharmProjects/FinetuningYOLOv8_Table/runs/detect/ft_roundII_upto50epochs/weights/best.pt, data=FT_data.yaml, epochs=18, patience=50, batch=10, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=0, project=None, name=ft_roundII_32to50epochs, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, min_memory=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, split=val, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.1, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/ft_roundII_32to50epochs\n",
      "2023-12-20 21:40:53.932791: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/suparna/anaconda3/envs/idp_0/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-12-20 21:40:53.933061: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.Conv                  [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.Conv                  [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.C2f                   [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.Conv                  [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.C2f                   [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.Conv                  [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.C2f                   [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.Conv                  [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.C2f                   [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.SPPF                  [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.C2f                   [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.C2f                   [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.Conv                  [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.C2f                   [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.Conv                  [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.C2f                   [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.Detect                [1, [192, 384, 576]]          \n",
      "Model summary: 295 layers, 25856899 parameters, 25856883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 475/475 items from pretrained weights\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0009375), 83 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/suparna/PycharmProjects/FinetuningYOLOv8_Table/data/train/\u001b[0m\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mWARNING âš ï¸ version 1.0.3 is required by YOLOv8, but version 0.5.2 is currently installed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/suparna/PycharmProjects/FinetuningYOLOv8_Table/data/valid/la\u001b[0m\n",
      "Plotting labels to runs/detect/ft_roundII_32to50epochs/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/ft_roundII_32to50epochs\u001b[0m\n",
      "Starting training for 18 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/18         0G      1.259      1.165      1.521         18        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.917      0.959      0.955      0.801\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/18         0G      1.119      1.051      1.445         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.919      0.931      0.957      0.783\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/18         0G      1.187      1.083      1.442         20        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.494      0.714      0.566       0.38\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/18         0G      1.228      1.099      1.512         25        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49        0.8      0.837      0.816      0.516\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/18         0G      1.352      1.162      1.589         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.905      0.973       0.98      0.842\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/18         0G      1.351      1.179      1.627         27        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.907      0.996      0.943      0.802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/18         0G      1.346       1.22      1.602         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49       0.91          1       0.97      0.836\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/18         0G      1.303       1.19      1.595         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.683      0.966      0.821      0.657\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mWARNING âš ï¸ version 1.0.3 is required by YOLOv8, but version 0.5.2 is currently installed\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/18         0G     0.9677     0.9828      1.376          9        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.524      0.743      0.538       0.38\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/18         0G     0.8726     0.8388      1.289          9        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.611      0.776      0.684      0.565\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/18         0G     0.9209     0.8178      1.299          6        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.611      0.633      0.675      0.556\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/18         0G     0.7746     0.7341      1.244          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.711      0.939      0.875      0.787\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/18         0G     0.7799     0.6932      1.237          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.776      0.939      0.882      0.793\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/18         0G     0.7779     0.7088      1.228         11        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.936          1       0.98      0.868\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/18         0G     0.7178     0.6232      1.193          9        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49       0.94      0.956      0.961      0.864\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/18         0G     0.7691      0.654      1.212          9        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.923      0.975      0.965      0.913\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/18         0G     0.7035     0.6344      1.172          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.889          1      0.972      0.934\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/18         0G     0.6709     0.6085      1.155          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.925       0.98      0.969      0.936\n",
      "\n",
      "18 epochs completed in 5.253 hours.\n",
      "Optimizer stripped from runs/detect/ft_roundII_32to50epochs/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from runs/detect/ft_roundII_32to50epochs/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating runs/detect/ft_roundII_32to50epochs/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.43 ðŸš€ Python-3.8.18 torch-2.1.1+cu121 CPU\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         49      0.925       0.98      0.969      0.936\n",
      "Speed: 2.1ms preprocess, 813.0ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/ft_roundII_32to50epochs\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/ft_roundII_32to50epochs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "!yolo task=detect mode=train model=/home/suparna/PycharmProjects/YOLO_finetuning_dec/yolov8m-table-extraction/best.pt imgsz=640 data=/home/suparna/PycharmProjects/YOLO_finetuning_dec/FT_data.yaml epochs=18 batch=10 name=bor_unBor_ft\n",
    "#/home/suparna/PycharmProjects/TableDetection/YOLO_FT_tables/runs/detect/keremberke8m_106/weights/best.pt this model was trained for 10 epoch, taking the best from the 10 epochs and finetuning further for 40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suparna/anaconda3/envs/idp_0/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Ultralytics YOLOv8.0.230 ðŸš€ Python-3.8.18 torch-2.1.1+cu121 CPU (Intel Core(TM) i5-7200U 2.50GHz)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/val/labels.\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/val/labels.cache\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         37         48      0.979          1      0.995      0.995\n",
      "Speed: 4.7ms preprocess, 1091.8ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/epoch100_eval3\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    }
   ],
   "source": [
    "#validation\n",
    "!yolo task=detect mode=val model=/home/suparna/PycharmProjects/YOLO_finetuning_dec/runs/detect/beauty_yolo_ft2/weights/best.pt name=epoch100_eval data=FT_data.yaml imgsz=640\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Predicting on test data1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suparna/anaconda3/envs/idp_0/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Ultralytics YOLOv8.0.230 ðŸš€ Python-3.8.18 torch-2.1.1+cu121 CPU (Intel Core(TM) i5-7200U 2.50GHz)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\n",
      "image 1/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/B_A_8.png: 640x480 1 Table, 668.6ms\n",
      "image 2/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/B_B_33.png: 640x416 1 Table, 530.4ms\n",
      "image 3/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/B_B_47.png: 640x416 1 Table, 530.5ms\n",
      "image 4/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_A_23.png: 640x512 1 Table, 692.0ms\n",
      "image 5/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_A_3.png: 640x512 1 Table, 826.6ms\n",
      "image 6/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_A_36.png: 640x512 1 Table, 763.2ms\n",
      "image 7/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_A_37.png: 640x512 1 Table, 818.1ms\n",
      "image 8/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_A_41.png: 640x512 1 Table, 783.5ms\n",
      "image 9/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_A_43.png: 640x512 1 Table, 828.5ms\n",
      "image 10/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_A_83.png: 640x512 1 Table, 829.1ms\n",
      "image 11/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_A_92.png: 640x512 1 Table, 763.8ms\n",
      "image 12/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_A_95.png: 640x512 1 Table, 754.0ms\n",
      "image 13/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_B_4.png: 384x640 1 Table, 539.6ms\n",
      "image 14/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_C_24.png: 640x480 1 Table, 826.0ms\n",
      "image 15/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_C_60.png: 640x480 1 Table, 698.3ms\n",
      "image 16/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_D_13.png: 640x416 1 Table, 626.0ms\n",
      "image 17/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_D_16.png: 640x416 1 Table, 693.1ms\n",
      "image 18/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_D_21.png: 640x416 1 Table, 585.8ms\n",
      "image 19/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_D_29.png: 640x416 1 Table, 604.0ms\n",
      "image 20/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_D_32.png: 640x416 1 Table, 617.3ms\n",
      "image 21/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_D_36.png: 640x416 1 Table, 649.1ms\n",
      "image 22/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_D_45.png: 640x416 1 Table, 671.0ms\n",
      "image 23/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/R_D_6.png: 640x416 1 Table, 674.1ms\n",
      "image 24/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-017.png: 640x512 2 Tables, 746.7ms\n",
      "image 25/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-028.png: 640x512 3 Tables, 837.0ms\n",
      "image 26/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-043.png: 640x480 1 Table, 662.7ms\n",
      "image 27/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-053.png: 640x480 3 Tables, 713.3ms\n",
      "image 28/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-079.png: 640x480 1 Table, 813.4ms\n",
      "image 29/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-086.png: 640x480 1 Table, 691.2ms\n",
      "image 30/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-093.png: 640x480 1 Table, 702.7ms\n",
      "image 31/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-094.png: 640x480 1 Table, 723.8ms\n",
      "image 32/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-099.png: 640x480 2 Tables, 764.5ms\n",
      "image 33/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-101.png: 640x480 1 Table, 1121.9ms\n",
      "image 34/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-103.png: 640x512 2 Tables, 1179.3ms\n",
      "image 35/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-109.png: 640x512 1 Table, 960.0ms\n",
      "image 36/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-130.png: 640x512 2 Tables, 903.3ms\n",
      "image 37/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-137.png: 640x512 2 Tables, 783.8ms\n",
      "image 38/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-147.png: 640x512 1 Table, 866.4ms\n",
      "image 39/39 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/combined_tables-150.png: 640x512 1 Table, 794.0ms\n",
      "Speed: 3.4ms preprocess, 749.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Results saved to \u001b[1mruns/detect/testpostgpu_100epochs\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=predict model=/home/suparna/PycharmProjects/YOLO_finetuning_dec/runs/detect/beauty_yolo_ft2/weights/best.pt imgsz=640 conf=0.25 iou=.45 agnostic_nms=False source=/home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images save=True name=testpostgpu_100epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on Vericred pdf images.02-01-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suparna/anaconda3/envs/idp_0/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Ultralytics YOLOv8.0.230 ðŸš€ Python-3.8.18 torch-2.1.1+cu121 CPU (Intel Core(TM) i5-7200U 2.50GHz)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\n",
      "image 1/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_0_1.png: 448x640 1 Table, 1325.9ms\n",
      "image 2/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_0_1_1.png: 448x640 1 Table, 1461.3ms\n",
      "image 3/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_0_2.png: 128x640 1 Table, 474.3ms\n",
      "image 4/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_0_2_1.png: 160x640 1 Table, 638.4ms\n",
      "image 5/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_1_1.png: 480x640 1 Table, 1160.6ms\n",
      "image 6/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_1_1_1.png: 480x640 1 Table, 928.4ms\n",
      "image 7/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_1_1_2.png: 224x640 1 Table, 536.5ms\n",
      "image 8/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_2_1.png: 512x640 1 Table, 934.0ms\n",
      "image 9/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_2_1_1.png: 512x640 1 Table, 1021.5ms\n",
      "image 10/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_3_1.png: 416x640 2 Tables, 810.3ms\n",
      "image 11/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_3_1_1.png: 416x640 2 Tables, 1032.2ms\n",
      "image 12/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_4_1.png: 96x640 1 Table, 307.7ms\n",
      "image 13/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_4_1_1.png: 96x640 1 Table, 257.6ms\n",
      "image 14/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_1.png: 384x640 1 Table, 685.0ms\n",
      "image 15/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_1_1.png: 384x640 1 Table, 851.7ms\n",
      "image 16/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_2.png: 416x640 1 Table, 666.8ms\n",
      "image 17/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_2_1.png: 320x640 3 Tables, 547.8ms\n",
      "image 18/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_2_2.png: 416x640 1 Table, 660.5ms\n",
      "image 19/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_2_3.png: 416x640 (no detections), 767.0ms\n",
      "image 20/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_2_4.png: 576x640 1 Table, 1157.0ms\n",
      "image 21/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_2_5.png: 128x640 3 Tables, 288.2ms\n",
      "image 22/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_3.png: 128x640 3 Tables, 297.2ms\n",
      "image 23/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred3sni_5_3_1.png: 128x640 3 Tables, 254.0ms\n",
      "image 24/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_0_1.png: 416x640 (no detections), 837.1ms\n",
      "image 25/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_0_2.png: 96x640 (no detections), 259.2ms\n",
      "image 26/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_1_1.png: 256x640 (no detections), 658.8ms\n",
      "image 27/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_1_1_1.png: 256x640 1 Table, 489.5ms\n",
      "image 28/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_1_1_2.png: 320x640 (no detections), 727.2ms\n",
      "image 29/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_1_2.png: 224x640 1 Table, 378.4ms\n",
      "image 30/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_2_1.png: 448x640 1 Table, 778.6ms\n",
      "image 31/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_2_1_1.png: 448x640 1 Table, 767.8ms\n",
      "image 32/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_3_1.png: 352x640 1 Table, 673.2ms\n",
      "image 33/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_4_1.png: 224x640 1 Table, 426.2ms\n",
      "image 34/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_4_1_1.png: 224x640 1 Table, 378.0ms\n",
      "image 35/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_5_1.png: 448x640 1 Table, 691.2ms\n",
      "image 36/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_5_1_1.png: 448x640 1 Table, 1130.0ms\n",
      "image 37/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_5_2.png: 192x640 1 Table, 352.4ms\n",
      "image 38/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_5_2_1.png: 192x640 1 Table, 341.9ms\n",
      "image 39/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_6_1.png: 224x640 3 Tables, 423.2ms\n",
      "image 40/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_7_1.png: 384x640 1 Table, 926.2ms\n",
      "image 41/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_7_1_1.png: 352x640 1 Table, 613.2ms\n",
      "image 42/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_7_1_2.png: 384x640 (no detections), 898.6ms\n",
      "image 43/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_7_1_3.png: 352x640 1 Table, 659.3ms\n",
      "image 44/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_7_1_4.png: 384x640 1 Table, 683.1ms\n",
      "image 45/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_7_1_5.png: 160x640 1 Table, 399.3ms\n",
      "image 46/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_7_2.png: 352x640 1 Table, 751.2ms\n",
      "image 47/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred7fmx_7_2_1.png: 256x640 (no detections), 543.4ms\n",
      "image 48/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_0_1.png: 256x640 1 Table, 472.0ms\n",
      "image 49/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_0_2.png: 64x640 (no detections), 205.7ms\n",
      "image 50/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_10_1.png: 288x640 1 Table, 542.6ms\n",
      "image 51/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_11_1.png: 224x640 (no detections), 512.5ms\n",
      "image 52/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_1_1.png: 288x640 1 Table, 725.2ms\n",
      "image 53/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_2_1.png: 480x640 2 Tables, 835.5ms\n",
      "image 54/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_3_1.png: 480x640 1 Table, 883.9ms\n",
      "image 55/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_3_2.png: 224x640 1 Table, 420.8ms\n",
      "image 56/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_4_1.png: 256x640 2 Tables, 460.8ms\n",
      "image 57/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_5_1.png: 480x640 1 Table, 812.3ms\n",
      "image 58/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_6_1.png: 416x640 2 Tables, 852.1ms\n",
      "image 59/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_8_1.png: 448x640 1 Table, 903.7ms\n",
      "image 60/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_8_2.png: 224x640 1 Table, 433.4ms\n",
      "image 61/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_8_3.png: 192x640 1 Table, 486.6ms\n",
      "image 62/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericred8q31_9_1.png: 416x640 (no detections), 774.1ms\n",
      "image 63/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_0_1.png: 480x640 (no detections), 1198.0ms\n",
      "image 64/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_1_1.png: 288x640 1 Table, 487.7ms\n",
      "image 65/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_2_1.png: 512x640 1 Table, 875.1ms\n",
      "image 66/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_3_1.png: 416x640 1 Table, 831.6ms\n",
      "image 67/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_4_1.png: 448x640 1 Table, 765.9ms\n",
      "image 68/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_5_1.png: 224x640 1 Table, 396.3ms\n",
      "image 69/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_6_1.png: 320x640 (no detections), 796.4ms\n",
      "image 70/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_6_2.png: 416x640 1 Table, 1024.3ms\n",
      "image 71/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_6_3.png: 352x640 (no detections), 561.1ms\n",
      "image 72/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredcsgl_6_4.png: 384x640 (no detections), 635.4ms\n",
      "image 73/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_0_1.png: 384x640 1 Table, 584.6ms\n",
      "image 74/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_1_1.png: 480x640 1 Table, 853.3ms\n",
      "image 75/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_2_1.png: 448x640 1 Table, 821.0ms\n",
      "image 76/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_3_1.png: 352x640 2 Tables, 724.1ms\n",
      "image 77/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_4_1.png: 192x640 (no detections), 381.8ms\n",
      "image 78/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_5_1.png: 448x640 1 Table, 895.0ms\n",
      "image 79/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_5_2.png: 224x640 (no detections), 664.0ms\n",
      "image 80/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_6_1.png: 480x640 1 Table, 875.6ms\n",
      "image 81/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_7_1.png: 480x640 2 Tables, 954.6ms\n",
      "image 82/82 /home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/Vericredkp8v_8_1.png: 448x640 1 Table, 903.2ms\n",
      "Speed: 3.8ms preprocess, 687.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns/detect/testpostgpu_100epochs2\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=predict model=/home/suparna/PycharmProjects/YOLO_finetuning_dec/runs/detect/beauty_yolo_ft2/weights/best.pt imgsz=640 conf=0.25 iou=.45 agnostic_nms=False source=/home/suparna/PycharmProjects/TableExtraction/data/cropped_tables save=True name=testpostgpu_100epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image,ImageOps\n",
    "import os\n",
    "\n",
    "# load model\n",
    "model = YOLO('/home/suparna/PycharmProjects/TableDetection/YOLO_FT_tables/best_model/keremberke8m_10_50/weights/best.pt')\n",
    "\n",
    "# set model parameters\n",
    "model.overrides['conf'] = 0.25  # NMS confidence threshold\n",
    "model.overrides['iou'] = 0.45  # NMS IoU threshold\n",
    "model.overrides['agnostic_nms'] = False  # NMS class-agnostic\n",
    "model.overrides['max_det'] = 1000  # maximum number of detections per image\n",
    "\n",
    "def detect_crop_save_tables(image):\n",
    "    print(image)\n",
    "    # set image\n",
    "    #image = \"/home/suparna/PycharmProjects/TableDetection/all_Data/FinQA_tables/combined_tables-056.png\"\n",
    "\n",
    "    # perform inference\n",
    "    results = model.predict(\"/home/suparna/PycharmProjects/TableDetection/all_Data/FinQA_tables/\"+image)\n",
    "    print(results)\n",
    "    # observe results\n",
    "    print(results[0].boxes)\n",
    "    # render = render_result(model=model, image=image, result=results[0])\n",
    "    # render.show()\n",
    "    file_name=\"enersys\"+image[-6:]\n",
    "    image_path=f'/home/suparna/PycharmProjects/TableDetection/all_Data/FinQA_tables'       \n",
    "        \n",
    "    #render=render.save(f'{image_path}/{file_name}' )\n",
    "    print(results[0].boxes.xyxy)\n",
    "    print(results[0].boxes.xyxyn)\n",
    "    print(results[0].boxes.xywh)\n",
    "    print(results[0].boxes.xywhn)\n",
    "\n",
    "    all_boxes=results[0].boxes.data.cpu().numpy()\n",
    "    for i, box in enumerate(all_boxes):\n",
    "        im2 = Image.open(image)\n",
    "        input_image = ImageOps.grayscale(im2) \n",
    "\n",
    "        x1, y1, x2, y2, conf, cls = box\n",
    "        tables = im2.crop((x1-10, y1-10, x2+10, y2+10))\n",
    "        \n",
    "        plate_filename = f'/home/suparna/PycharmProjects/TableDetection/Approaches_results/enersys_cropped_tables/{image[-7:]}_cropped_margin10_{i+1}_.png'\n",
    "        tables.save(plate_filename)\n",
    "\n",
    "\n",
    "images=os.listdir(\"/home/suparna/PycharmProjects/TableDetection/all_Data/FinQA_tables\")\n",
    "for img in images:\n",
    "    print(img)\n",
    "    detect_crop_save_tables(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/suparna/PycharmProjects/YOLO_finetuning_dec/data/test/images/B_A_8.png: 640x480 1 Table, 582.6ms\n",
      "Speed: 3.7ms preprocess, 582.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "boxes\n",
      "tensor([[ 40.7488, 115.6442, 548.7826, 750.3737]])\n",
      "tensor([[0.0684, 0.1373, 0.9208, 0.8912]])\n",
      "tensor([[294.7657, 433.0089, 508.0338, 634.7295]])\n",
      "tensor([[0.4946, 0.5143, 0.8524, 0.7538]])\n",
      "all boxes\n",
      "[[     40.749      115.64      548.78      750.37     0.98284           0]]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "# from image_processing.remove_horizontal_lines import remove_h_line\n",
    "# from image_processing.dialate_vertical_space import add_v_space\n",
    "# from image_processing.dialate_horizontal_space import add_h_space\n",
    "\n",
    "model_path=\"/home/suparna/PycharmProjects/TableExtraction/model/beauty_yolo_ft3_150/weights/best.pt\"\n",
    "markedtable_path=\"/home/suparna/PycharmProjects/TableExtraction/data/tables_marked/\"\n",
    "cropped_image_path=\"/home/suparna/PycharmProjects/TableExtraction/data/cropped_tables/\"\n",
    "coords_txt_path=\"/home/suparna/PycharmProjects/TableExtraction/data/coordinates/\"\n",
    "# load model\n",
    "#model = YOLO('yolov8n.pt')  # load an official model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# set model parameters\n",
    "model.overrides['conf'] = 0.25  # NMS confidence threshold\n",
    "model.overrides['iou'] = 0.45  # NMS IoU threshold\n",
    "model.overrides['agnostic_nms'] = False  # NMS class-agnostic\n",
    "model.overrides['max_det'] = 1000  # maximum number of detections per image\n",
    "\n",
    "\n",
    "def detect_table(img:str)-> None:\n",
    "    '''\n",
    "    Args:\n",
    "        img: Complete page image\n",
    "    Returns:None\n",
    "    steps:\n",
    "    1. detects the table/tables present in the image\n",
    "    2. saves the coordinates in coords_txt_path in .txt file with the same as that of the img.\n",
    "    3. also invokes crop table to crop the image and save in cropped_image_path\n",
    "    '''\n",
    "\n",
    "    # 1. detects the table/tables present in the image\n",
    "    image=img\n",
    "    file_name = image.split('/')[-1]\n",
    "\n",
    "    results = model.predict(image)\n",
    "    #render = render_result(model=model, image=image, result=results[0])\n",
    "    #render.show()\n",
    "\n",
    "    #2. saves the coordinates in coords_txt_path in .txt file with the same as that of the img.\n",
    "    file_name = image.split('/')[-1]\n",
    "    #render = render.save(f'{markedtable_path}{file_name}')\n",
    "    #render.show()\n",
    "    print(\"boxes\")\n",
    "    print(results[0].boxes.xyxy)\n",
    "    print(results[0].boxes.xyxyn)\n",
    "    print(results[0].boxes.xywh)\n",
    "    print(results[0].boxes.xywhn)\n",
    "\n",
    "    #3. also invokes crop table to crop the image and save in cropped_image_path\n",
    "    crop_table(results,image)\n",
    "\n",
    "\n",
    "def crop_table(results,image:str) -> None:\n",
    "    '''    \n",
    "    Args:\n",
    "        results:list of tensors as detected by the prediction model\n",
    "        image: path to image\n",
    "\n",
    "    Returns:None\n",
    "\n",
    "    '''\n",
    "    all_boxes = results[0].boxes.data.cpu().numpy()\n",
    "    print(\"all boxes\")\n",
    "    print(all_boxes)\n",
    "    box_coord=[]\n",
    "    for i, box in enumerate(all_boxes):\n",
    "        im2 = Image.open(image)\n",
    "        input_image = ImageOps.grayscale(im2)\n",
    "\n",
    "        #cropping the tables with an added margin of 20 pixels\n",
    "        x1, y1, x2, y2, conf, cls = box\n",
    "        tables = input_image.crop((x1 - 20, y1 - 20, x2 +20, y2 +20))\n",
    "\n",
    "        table_filename = (f'{cropped_image_path}{image.split(\"/\")[-1][:-4]}_{i + 1}.png')\n",
    "        box_coord.append([int(x1),int(y1),int(x2),int(y2)])\n",
    "\n",
    "        tables.save(table_filename)\n",
    "        #print(box_coord)\n",
    "\n",
    "    coordsfname=f\"{coords_txt_path}{image.split('/')[-1][:-4]}.txt\"\n",
    "    with open(coordsfname, 'w') as f:\n",
    "        c=str(box_coord)\n",
    "        f.write(c)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    img=\"data/test/images/B_A_8.png\" #only image name\n",
    "    detect_table(img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tatras_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
